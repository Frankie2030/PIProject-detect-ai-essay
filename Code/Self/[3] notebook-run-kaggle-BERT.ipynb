{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Install Package"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:32:04.231228Z","iopub.status.busy":"2024-10-11T11:32:04.230113Z"},"trusted":true},"outputs":[],"source":["%pip install scipy seaborn scikit-learn tensorflow tensorflow_datasets \n","%pip install pandas\n","%pip install matplotlib\n","%pip install numpy\n","%pip install sklearn\n","%pip install xgboost\n","%pip install gensim\n","%pip install nltk\n","%pip install tqdm\n","%pip install pyvi\n","%pip install tensorflow\n","%pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !rm -rf /kaggle/working/*"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Imports\n","from tqdm import tqdm\n","import os\n","import time\n","import re\n","import joblib\n","import torch\n","import logging\n","import nltk\n","from nltk import word_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","import xgboost as xgb\n","from xgboost import XGBClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, log_loss, hinge_loss\n","from datasets import load_dataset, Dataset\n","from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","from gensim.test.utils import datapath, get_tmpfile\n","from pyvi import ViTokenizer, ViPosTagger\n","from tqdm import tqdm\n","from sklearn.svm import LinearSVC\n","from sklearn.utils.class_weight import compute_class_weight\n","import tensorflow as tf\n","\n","# ref: https://www.kaggle.com/code/eswarbabu88/toxic-comment-glove-logistic-regression\n","# need to use glove_model from above\n","# Download required NLTK resources\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{},"source":["# Cute processing functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ===========================================================================================\n","# Tạo class Logger\n","class MyLogger:\n","    def __init__(self, log_file='app.log'):\n","        self.log_file = log_file\n","        self._initialize_logger()\n","\n","    def _initialize_logger(self):\n","        # Check if the log file already exists; if so, append to it\n","        if os.path.exists(self.log_file):\n","            file_mode = 'a'  # Append mode\n","        else:\n","            file_mode = 'w'  # Write mode (create new file)\n","\n","        # Create a logger\n","        self.logger = logging.getLogger()\n","        self.logger.setLevel(logging.INFO)  # Set logging level to INFO\n","\n","        # Create file handler\n","        file_handler = logging.FileHandler(self.log_file, mode=file_mode, encoding='utf-8')\n","        file_handler.setLevel(logging.INFO)  # Ensure the file handler logs INFO and above\n","\n","        # Create console handler\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(logging.INFO)  # Ensure the console handler logs INFO and above\n","\n","        # Set up the logging format\n","        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n","        file_handler.setFormatter(formatter)\n","        console_handler.setFormatter(formatter)\n","\n","        # Remove existing handlers to avoid duplicates\n","        if self.logger.hasHandlers():\n","            self.logger.handlers.clear()\n","\n","        # Add handlers to the logger\n","        self.logger.addHandler(file_handler)\n","        self.logger.addHandler(console_handler)\n","\n","    def log_message(self, message):\n","        self.logger.info(message)\n","\n","    def change_log_file(self, new_log_file):\n","        \"\"\"Change the log file and reinitialize the logger.\"\"\"\n","        self.log_file = new_log_file\n","        self._initialize_logger()  # Reinitialize logger with the new log file\n","\n","# ============================================================================================\n","# danh sách các biến toàn cục\n","logger = MyLogger()\n","\n","# Enable/Disable tokenizers parallelism to avoid the warning\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n","\n","# ============================================================================================\n","# Danh sách các hàm xử lý\n","# ============================================================================================\n","# Load the stop words\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to remove stop words\n","def remove_stop_words(text):\n","    # Tokenize the text\n","    words = word_tokenize(text)\n","    \n","    # Remove stop words\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","    \n","    # Join the filtered words back into a string\n","    return ' '.join(filtered_words)\n","\n","def preprocess_text(text):\n","    # Step 1: Remove URLs\n","    text = re.sub(r'http\\S+|https?://\\S+|www\\.\\S+', '', text)\n","    \n","    # Step 2: Remove text in square brackets\n","    text = re.sub(r'\\[.*?\\]', '', text)\n","    \n","    # Step 3: Remove angle brackets\n","    text = re.sub(r'<.*?>+', '', text)\n","    \n","    # Step 4: Remove newlines, tabs, carriage returns, form feeds, backspace characters\n","    text = re.sub(r'\\n|\\t|\\r|\\f|\\b', '', text)\n","    \n","    # Step 5: Remove words that contain numbers\n","    text = re.sub(r'\\w*\\d\\w*', '', text)\n","    \n","    # Step 6: Remove any non-alphanumeric characters, then make lowercase\n","    text = re.sub(r'\\W+', ' ', text).lower().strip()\n","    \n","    # Step 7: Tokenize the Vietnamese text using ViTokenizer\n","    text = ViTokenizer.tokenize(text)\n","    \n","    text = remove_stop_words(text)\n","    \n","    return text\n","\n","def load_data(file_path):\n","    # Load the dataset\n","    df_ds = pd.read_csv(file_path)\n","\n","    # First, split into train and test sets (80% train, 20% test)\n","    train_essays, test_essays = train_test_split(df_ds, test_size=0.2, random_state=42)\n","\n","    # Then, split the train set into train and validation sets (67% train, 33% validation)\n","    train_essays, val_essays = train_test_split(train_essays, test_size=0.33, random_state=42)\n","    \n","    return df_ds, train_essays, test_essays, val_essays\n","\n","def compute_metrics(preds, labels):\n","    # Convert probabilities to binary predictions\n","    binary_preds = (preds >= 0.5).astype(int)\n","\n","    # Compute ROC AUC score\n","    auc = roc_auc_score(labels, preds)\n","\n","    # Other metrics with zero_division set to handle undefined metrics\n","    accuracy = accuracy_score(labels, binary_preds)\n","    precision = precision_score(labels, binary_preds, zero_division=0)  # Use zero_division=0 to avoid warnings\n","    recall = recall_score(labels, binary_preds)\n","    f1 = f1_score(labels, binary_preds, zero_division=0)  # Use zero_division=0 to avoid warnings\n","\n","    return {\"roc_auc\": auc, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","def compute_metrics_bert(eval_pred):\n","    logits, labels = eval_pred\n","    # Convert logits to probabilities using softmax\n","    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n","    \n","    # Get the predicted class by selecting the class with the highest probability\n","    preds = np.argmax(probs, axis=-1)\n","    \n","    # Compute ROC AUC score using probabilities of the positive class (class 1)\n","    if len(np.unique(labels)) > 1:  # Ensure there are both classes present for AUC computation\n","        auc = roc_auc_score(labels, probs[:, 1])\n","    else:\n","        auc = float('nan')  # AUC is not defined if only one class is present\n","    \n","    # Compute other metrics\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, zero_division=0)\n","    recall = recall_score(labels, preds)\n","    f1 = f1_score(labels, preds, zero_division=0)\n","\n","    return {\"roc_auc\": auc, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","# def compute_metrics_bert(logits, labels):\n","    # Convert logits to probabilities using softmax\n","    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n","    \n","    # Get the predicted class by selecting the class with the highest probability\n","    preds = np.argmax(probs, axis=-1)\n","    \n","    # Convert one-hot encoded labels to single class labels if necessary\n","    if len(labels.shape) > 1 and labels.shape[1] > 1:\n","        labels = np.argmax(labels, axis=-1)\n","\n","    # Compute ROC AUC score using probabilities of the positive class (class 1)\n","    if len(np.unique(labels)) > 1:  # Ensure there are both classes present for AUC computation\n","        auc = roc_auc_score(labels, probs[:, 1])  # For binary classification\n","    else:\n","        auc = float('nan')  # AUC is not defined if only one class is present\n","    \n","    # Compute other metrics\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average='binary', zero_division=0)\n","    recall = recall_score(labels, preds, average='binary')\n","    f1 = f1_score(labels, preds, average='binary', zero_division=0)\n","\n","    return {\"roc_auc\": auc, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","def plot_tsne(model, num):\n","    labels = []\n","    tokens = []\n","    for word in model.key_to_index:\n","        if word not in stop_words:\n","            tokens.append(np.array(model[word]))\n","            labels.append(word)\n","    tsne = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 2500, random_state = 23)\n","    data = tsne.fit_transform(np.array(tokens[:num]))\n","    x = []\n","    y = []\n","    for each in data:\n","        x.append(each[0])\n","        y.append(each[1])\n","    plt.figure(figsize = (10, 10))\n","    for i in range(num):\n","        plt.scatter(x[i], y[i])\n","        plt.annotate(labels[i],\n","                     xy = (x[i], y[i]),\n","                     xytext = (5,2),\n","                     textcoords = 'offset points',\n","                     ha = 'right',\n","                     va = 'bottom')\n","    plt.show()\n","\n","# Assuming glove_model is already loaded in your environment\n","# Function to convert a sentence to a vector\n","def sent2vec(s, glove_model):\n","    words = str(s).lower()\n","    words = word_tokenize(words)  # This requires the 'punkt' tokenizer\n","    words = [w for w in words if w not in stop_words]\n","    words = [w for w in words if w.isalpha()]  # Filter out non-alphabetic tokens\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(glove_model[w])  # Lookup word in GloVe model\n","        except KeyError:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    if type(v) != np.ndarray:\n","        return np.zeros(300)  # Return a zero vector if no word embeddings are found\n","    return v / np.sqrt((v ** 2).sum())  # Normalize the vector\n","\n","# ============================================================================================\n","def plot_training_validation_curves(train_metrics, val_metrics, save_path, file_name):\n","    # Create the output directory if it doesn't exist\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","\n","    # Plot accuracy and loss\n","    plt.figure(figsize=(12, 5))\n","\n","    # Plot accuracy\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_metrics['accuracy'], label='Train Accuracy')\n","    plt.plot(val_metrics['accuracy'], label='Validation Accuracy')\n","    plt.title('Accuracy over epochs')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    # Plot loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_metrics['loss'], label='Train Loss')\n","    plt.plot(val_metrics['loss'], label='Validation Loss')\n","    plt.title('Loss over epochs')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","\n","    # Save the plot to the specified directory with the specified file name\n","    plot_path = os.path.join(save_path, file_name)\n","    plt.savefig(plot_path)\n","\n","    # Show the plot as well\n","    plt.show()\n","\n","    print(f\"Plot saved to: {plot_path}\")\n","\n","# Custom function to calculate binary cross-entropy loss\n","def binary_cross_entropy_loss(y_true, y_pred):\n","    epsilon = 1e-15  # Small constant to avoid log(0)\n","    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predictions to avoid issues with log(0)\n","    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n","    return loss\n","\n","def test_model(X_test_embeddings, y_test, model_file_path):\n","    # Load the trained model\n","    model = joblib.load(model_file_path)\n","    \n","    # Predict probabilities\n","    preds_test = model.predict_proba(X_test_embeddings)[:, 1]\n","    \n","    # Compute test metrics\n","    test_loss = binary_cross_entropy_loss(y_test, preds_test)\n","    test_metrics = compute_metrics(preds_test, y_test)\n","\n","    # Log test metrics\n","    logger.log_message(\"Testing Results:\")\n","    logger.log_message(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \")\n","    logger.log_message(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \")\n","    logger.log_message(f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n","    \n","    # Print test metrics for console\n","    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \"\n","          f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \"\n","          f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n","    \n","    # Return all test metrics\n","    return {\"loss\": test_loss, **test_metrics}\n","\n","def test_linearsvc(X_test_embeddings, y_test, model_file_path): \n","    # Load the trained model\n","    model = joblib.load(model_file_path)\n","    \n","    # Predict decision function values (not probabilities) for the test set\n","    preds_test = model.decision_function(X_test_embeddings)\n","\n","    # Calculate hinge loss (suitable for SVM-based models like LinearSVC)\n","    test_loss = hinge_loss(y_test, preds_test)\n","    \n","    # Convert decision function values to binary predictions (-1, 1 -> 0, 1)\n","    preds_test_binary = (preds_test > 0).astype(int)\n","\n","    # Compute test metrics\n","    test_accuracy = accuracy_score(y_test, preds_test_binary)\n","    test_precision = precision_score(y_test, preds_test_binary)\n","    test_recall = recall_score(y_test, preds_test_binary)\n","    test_f1 = f1_score(y_test, preds_test_binary)\n","    test_roc_auc = roc_auc_score(y_test, preds_test_binary)\n","\n","    # Log test metrics\n","    logger.log_message(\"Testing Results:\")\n","    logger.log_message(f\"Test Loss (Hinge Loss): {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \")\n","    logger.log_message(f\"Test ROC AUC: {test_roc_auc:.4f}, Precision: {test_precision:.4f}, \")\n","    logger.log_message(f\"Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n","    \n","    # Print test metrics for console\n","    print(f\"Test Loss (Hinge Loss): {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n","          f\"Test ROC AUC: {test_roc_auc:.4f}, Precision: {test_precision:.4f}, \"\n","          f\"Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n","    \n","    # Return all test metrics\n","    return {\n","        \"loss\": test_loss,\n","        \"accuracy\": test_accuracy,\n","        \"precision\": test_precision,\n","        \"recall\": test_recall,\n","        \"f1\": test_f1,\n","        \"roc_auc\": test_roc_auc\n","    }\n","\n","def test_rnn(X_test, y_test, model_file_path):\n","    \"\"\"\n","    Function to test a trained RNN model using TensorFlow/Keras.\n","    It loads the model, performs predictions, and computes metrics.\n","    \"\"\"\n","    # Load the trained model\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model from file...\")\n","        model = tf.keras.models.load_model(model_file_path)\n","    else:\n","        raise FileNotFoundError(f\"Model file not found at {model_file_path}\")\n","\n","    # Create a TensorFlow dataset for testing\n","    batch_size = 32\n","    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n","\n","    # Predict logits (raw output before activation)\n","    preds_test_logits = model.predict(test_dataset)\n","\n","    # Convert logits to probabilities using sigmoid since we are doing binary classification\n","    preds_test = tf.sigmoid(preds_test_logits).numpy().flatten()\n","\n","    # Compute test loss using log loss (cross-entropy)\n","    test_loss = log_loss(y_test, preds_test)\n","\n","    # Compute test metrics (precision, recall, F1, etc.)\n","    test_metrics = compute_metrics(preds_test, y_test)\n","\n","    # Log test metrics\n","    logger.log_message(\"Testing Results:\")\n","    logger.log_message(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \")\n","    logger.log_message(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \")\n","    logger.log_message(f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n","\n","    # Print test metrics to console\n","    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \"\n","          f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \"\n","          f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n","\n","    # Return test loss and all test metrics\n","    return {\"loss\": test_loss, **test_metrics}\n","\n","def test_distilbert(X_test, model_file_path):\n","    \"\"\"\n","    Function to test a DistilBERT model on the test dataset and print metrics.\n","    \"\"\"\n","    # Load the trained model and tokenizer\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model and tokenizer from file...\")\n","        model = AutoModelForSequenceClassification.from_pretrained(model_file_path)\n","        tokenizer = AutoTokenizer.from_pretrained(model_file_path)\n","    else:\n","        raise FileNotFoundError(f\"Model file not found at {model_file_path}\")\n","\n","    # Create the test dataset\n","    test_dataset = Dataset.from_pandas(X_test)\n","\n","    # Tokenize the test dataset\n","    tokenized_test = test_dataset.map(lambda examples: tokenizer(examples['processed_text_swr'], max_length=128, padding=True, truncation=True), batched=True)\n","\n","    # Initialize the Trainer (no training arguments needed for testing)\n","    trainer = Trainer(model=model, tokenizer=tokenizer)\n","\n","    # Perform predictions on the test set\n","    predictions = trainer.predict(tokenized_test)\n","    preds_test = predictions.predictions.argmax(axis=-1)\n","\n","    # Compute metrics for the test set using the logits and labels from predictions\n","    test_metrics = compute_metrics_bert((predictions.predictions, predictions.label_ids))\n","\n","    # Log and print the test metrics\n","    logger.log_message(\"Testing Results:\")\n","    logger.log_message(f\"Test Accuracy: {test_metrics['accuracy']:.4f}, Test ROC AUC: {test_metrics['roc_auc']:.4f}, \")\n","    logger.log_message(f\"Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n","    \n","    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}, Test ROC AUC: {test_metrics['roc_auc']:.4f}, \"\n","          f\"Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n","\n","    return test_metrics\n","\n","\n","# ============================================================================================\n","def shuffle_and_split(X, y, batch_size):\n","    \"\"\"Shuffle the data and split into batches.\"\"\"\n","    indices = np.random.permutation(len(y))  # Shuffle the indices\n","    X_shuffled = X[indices]\n","    y_shuffled = y[indices]\n","    \n","    # Divide into batches\n","    num_batches = len(y) // batch_size\n","    X_batches = np.array_split(X_shuffled, num_batches)\n","    y_batches = np.array_split(y_shuffled, num_batches)\n","    \n","    return X_batches, y_batches\n","\n","#######\n","def train_logistic_regression(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n","    logger.log_message(f\"Training a Logistic Regression model for {n_epochs} epochs...\")\n","\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model from file...\")\n","        model = joblib.load(model_file_path)\n","    else:\n","        print(\"Model not found. Training a new model...\")\n","        model = LogisticRegression(max_iter=246, C=0.01, class_weight='balanced')\n","\n","        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","        # Loop over epochs\n","        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n","            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n","\n","            # Shuffle and split data into batches\n","            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n","\n","            # Train model on each batch\n","            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n","                model.fit(X_batch, y_batch)\n","\n","            # After training on batches, validate\n","            preds_train = model.predict_proba(X_train_embeddings)[:, 1]  # Probability of positive class\n","            preds_val = model.predict_proba(X_val_embeddings)[:, 1]\n","\n","            # Compute training loss using binary cross-entropy\n","            train_loss = binary_cross_entropy_loss(y_train, preds_train)\n","            train_metrics_epoch = compute_metrics(preds_train, y_train)\n","            train_metrics['loss'].append(train_loss)\n","            for key, value in train_metrics_epoch.items():\n","                train_metrics[key].append(value)\n","\n","            # Compute validation metrics and loss using binary cross-entropy\n","            val_loss = binary_cross_entropy_loss(y_val, preds_val)\n","            val_metrics_epoch = compute_metrics(preds_val, y_val)\n","            val_metrics['loss'].append(val_loss)\n","            for key, value in val_metrics_epoch.items():\n","                val_metrics[key].append(value)\n","\n","            # Print metrics for the epoch\n","            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n","            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n","            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","            \n","            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n","            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n","\n","        # Save the trained model\n","        joblib.dump(model, model_file_path)\n","        print(f\"Model saved to {model_file_path}\")\n","\n","        # Plot accuracy and loss curves\n","        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"LogisticRegressionAnalysis.png\")\n","\n","    return model\n","\n","#######\n","def train_xgboost(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n","    logger.log_message(f\"Training an XGBoost model for {n_epochs} epochs...\")\n","\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model from file...\")\n","        model = joblib.load(model_file_path)\n","    else:\n","        print(\"Model not found. Training a new model...\")\n","        # model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.01, n_estimators=100)\n","        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.01, n_estimators=100)\n","\n","        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","        # Loop over epochs\n","        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n","            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n","\n","            # Shuffle and split data into batches\n","            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n","\n","            # Train model on each batch\n","            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n","                model.fit(X_batch, y_batch)\n","\n","            # After training on batches, validate\n","            preds_train = model.predict_proba(X_train_embeddings)[:, 1]\n","            preds_val = model.predict_proba(X_val_embeddings)[:, 1]\n","\n","            # Compute training metrics\n","            train_loss = binary_cross_entropy_loss(y_train, preds_train)\n","            train_metrics_epoch = compute_metrics(preds_train, y_train)\n","            train_metrics['loss'].append(train_loss)\n","            for key, value in train_metrics_epoch.items():\n","                train_metrics[key].append(value)\n","\n","            # Compute validation metrics\n","            val_loss = binary_cross_entropy_loss(y_val, preds_val)\n","            val_metrics_epoch = compute_metrics(preds_val, y_val)\n","            val_metrics['loss'].append(val_loss)\n","            for key, value in val_metrics_epoch.items():\n","                val_metrics[key].append(value)\n","\n","            # Print metrics for the epoch\n","            # logger.log_message(f\"Epoch {epoch + 1}/{n_epochs}\")\n","            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n","            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n","            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","\n","            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n","            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n","\n","        # Save the trained model\n","        joblib.dump(model, model_file_path)\n","        print(f\"Model saved to {model_file_path}\")\n","\n","        # Plot accuracy and loss curves\n","        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"xgboost_training_validation_curves.jpg\")\n","\n","    return model\n","\n","#######\n","def train_random_forest(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n","    logger.log_message(f\"Training a Random Forest model for {n_epochs} epochs...\")\n","\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model from file...\")\n","        model = joblib.load(model_file_path)\n","    else:\n","        print(\"Model not found. Training a new model...\")\n","        model = RandomForestClassifier(n_estimators=24, max_depth=10, random_state=42)\n","\n","        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","        # Loop over epochs\n","        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n","            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n","\n","            # Shuffle and split data into batches\n","            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n","\n","            # Train model on each batch\n","            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n","                start = time.time()\n","                model.fit(X_batch, y_batch)\n","                # print(f\"Model fit time: {time.time() - start} seconds\")\n","\n","            # After training on batches, validate\n","            preds_train = model.predict_proba(X_train_embeddings)[:, 1]\n","            preds_val = model.predict_proba(X_val_embeddings)[:, 1]\n","\n","            # Compute training metrics\n","            train_loss = binary_cross_entropy_loss(y_train, preds_train)\n","            train_metrics_epoch = compute_metrics(preds_train, y_train)\n","            train_metrics['loss'].append(train_loss)\n","            for key, value in train_metrics_epoch.items():\n","                train_metrics[key].append(value)\n","\n","            # Compute validation metrics\n","            val_loss = binary_cross_entropy_loss(y_val, preds_val)\n","            val_metrics_epoch = compute_metrics(preds_val, y_val)\n","            val_metrics['loss'].append(val_loss)\n","            for key, value in val_metrics_epoch.items():\n","                val_metrics[key].append(value)\n","\n","            # Print metrics for the epoch\n","            # logger.log_message(f\"Epoch {epoch + 1}/{n_epochs}\")\n","            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n","            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n","            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","\n","            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n","            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n","\n","        # Save the trained model\n","        joblib.dump(model, model_file_path)\n","        print(f\"Model saved to {model_file_path}\")\n","\n","        # Plot accuracy and loss curves\n","        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"RF_training_validation_curves.jpg\")\n","\n","    return model\n","\n","#######\n","def train_linear_svc(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n","    logger.log_message(f\"Training a LinearSVC model for {n_epochs} epochs...\")\n","\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model from file...\")\n","        model = joblib.load(model_file_path)\n","    else:\n","        print(\"Model not found. Training a new model...\")\n","        # Instantiate the LinearSVC model\n","        model = LinearSVC(max_iter=246, class_weight='balanced', tol=1e-4, C=1.0)\n","\n","        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","        # Loop over epochs\n","        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n","            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n","\n","            # Shuffle and split data into batches\n","            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n","\n","            # Train model on each batch\n","            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n","                model.fit(X_batch, y_batch)\n","\n","            # After training on batches, validate\n","            preds_train = model.decision_function(X_train_embeddings)\n","            preds_val = model.decision_function(X_val_embeddings)\n","\n","            # Compute training metrics\n","            train_loss = hinge_loss(y_train, preds_train)\n","            train_metrics_epoch = compute_metrics(preds_train, y_train)\n","            train_metrics['loss'].append(train_loss)\n","            for key, value in train_metrics_epoch.items():\n","                train_metrics[key].append(value)\n","\n","            # Compute validation metrics\n","            val_loss = hinge_loss(y_val, preds_val)\n","            val_metrics_epoch = compute_metrics(preds_val, y_val)\n","            val_metrics['loss'].append(val_loss)\n","            for key, value in val_metrics_epoch.items():\n","                val_metrics[key].append(value)\n","\n","            # Print metrics for the epoch\n","            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n","            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n","            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","\n","            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n","            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n","            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n","                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n","                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n","\n","        # Save the trained model\n","        joblib.dump(model, model_file_path)\n","        print(f\"Model saved to {model_file_path}\")\n","\n","        # Plot accuracy and loss curves\n","        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"LinearSVC_training_validation_curves.png\")\n","\n","    return model\n","\n","#######\n","def train_rnn(X_train, y_train, X_val, y_val, model_file_path, plot_file_path, n_epochs=20, batch_size=32):\n","    \"\"\"\n","    Function to train a Bidirectional LSTM RNN using TensorFlow/Keras.\n","    It checks if the model already exists, otherwise it trains and saves the model.\n","    \"\"\"\n","    # Create TensorFlow datasets from the input data\n","    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n","    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n","\n","    # Check if the model already exists\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model from file...\")\n","        model = tf.keras.models.load_model(model_file_path)\n","    else:\n","        print(\"Model not found. Training a new model...\")\n","\n","        # Text vectorization layer\n","        VOCAB_SIZE = 1000\n","        encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n","        encoder.adapt(train_dataset.map(lambda text, label: text))\n","\n","        # Define the Bidirectional LSTM RNN model\n","        model = tf.keras.Sequential([\n","            encoder,\n","            tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True),\n","            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","            tf.keras.layers.Dense(64, activation='relu'),\n","            tf.keras.layers.Dense(1)\n","        ])\n","\n","        # Compile the model\n","        model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","                      optimizer=tf.keras.optimizers.Adam(1e-4),\n","                      metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n","\n","        # Train the model\n","        history = model.fit(train_dataset, epochs=n_epochs, validation_data=val_dataset)\n","\n","        # Save the model\n","        model.save(model_file_path)\n","        print(f\"Model saved to {model_file_path}\")\n","\n","        # Plot training and validation accuracy and loss\n","        def plot_graphs(history, metric):\n","            plt.plot(history.history[metric])\n","            plt.plot(history.history['val_' + metric], '')\n","            plt.xlabel(\"Epochs\")\n","            plt.ylabel(metric)\n","            plt.legend([metric, 'val_' + metric])\n","\n","        plt.figure(figsize=(16, 8))\n","        plt.subplot(1, 2, 1)\n","        plot_graphs(history, 'accuracy')\n","        plt.ylim(None, 1)\n","        plt.subplot(1, 2, 2)\n","        plot_graphs(history, 'loss')\n","        plt.ylim(0, None)\n","\n","        # Save the plot\n","        plt.savefig(plot_file_path)\n","        print(f\"Training plot saved to {plot_file_path}\")\n","        plt.show()\n","\n","    return model\n","\n","#######\n","def preprocess_function(examples, tokenizer):\n","    return tokenizer(examples[\"processed_text_swr\"], max_length=128, padding=True, truncation=True)\n","\n","def plot_graphs(log_history, metric):\n","    epochs = [entry['epoch'] for entry in log_history if metric in entry]\n","    metric_values = [entry[metric] for entry in log_history if metric in entry]\n","    plt.plot(epochs, metric_values)\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(metric)\n","    plt.legend([metric])\n","\n","def train_distilbert(df_ds, model_file_path, image_file_path, n_epochs=20):\n","    \"\"\"\n","    Function to train a DistilBERT model. \n","    It checks if the model already exists, otherwise it trains and saves the model.\n","    \"\"\"\n","    # Check if model exists\n","    if os.path.exists(model_file_path):\n","        print(\"Loading model from file...\")\n","        model = AutoModelForSequenceClassification.from_pretrained(model_file_path)\n","    else:\n","        print(\"Model not found. Training a new model...\")\n","\n","        # Split dataset\n","        train_essays, test_essays = train_test_split(df_ds, test_size=0.2, random_state=42)\n","        train_essays, val_essays = train_test_split(train_essays, test_size=0.33, random_state=42)\n","\n","        # Tokenizer and model\n","        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n","        # Create dataset\n","        train_essay_dataset = Dataset.from_pandas(train_essays)\n","        val_essay_dataset = Dataset.from_pandas(val_essays)\n","        \n","        # Tokenize datasets, passing the tokenizer as an argument to the map function\n","        tokenized_train_essays = train_essay_dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True)\n","        tokenized_val_essays = val_essay_dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True)\n","\n","        # Define training arguments\n","        training_args = TrainingArguments(\n","            output_dir=\"/kaggle/working/\",\n","            learning_rate=2e-5,\n","            num_train_epochs=n_epochs,\n","            weight_decay=0.01,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            report_to='none'\n","        )\n","\n","        # Initialize Trainer\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_train_essays,\n","            eval_dataset=tokenized_val_essays,\n","            tokenizer=tokenizer,\n","            compute_metrics=compute_metrics_bert\n","        )\n","\n","        # Train the model\n","        trainer.train()\n","\n","        # Save the trained model\n","        model.save_pretrained(model_file_path)  # Updated to Hugging Face save method\n","        tokenizer.save_pretrained(model_file_path)  # Save tokenizer too\n","        print(f\"Model saved to {model_file_path}\")\n","\n","        # Plot accuracy and loss graphs\n","        plt.figure(figsize=(16, 8))\n","        plt.subplot(1, 2, 1)\n","        plot_graphs(trainer.state.log_history, 'eval_accuracy')\n","        plt.ylim(None, 1)\n","        plt.subplot(1, 2, 2)\n","        plot_graphs(trainer.state.log_history, 'eval_loss')\n","        plt.ylim(0, None)\n","\n","        # Save the plot\n","        plt.savefig(image_file_path)\n","        plt.show()\n","        print(f\"Training plot saved as {image_file_path}\")\n","\n","    return model\n","\n","\n","# ============================================================================================\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Main Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def main():\n","    # Khởi tạo số luồng xử lý song song\n","    # max_workers = 1 \n","    \n","    # kaggle \n","    in_base_path = r\"/kaggle/input/dath-pdz/\"\n","    out_base_path = r\"/kaggle/working/\"\n","    \n","    # in_base_path = r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK241\\CO3101_Do_an_Tri_tue_nhan_tao\\Main\\Dataset\"\n","    # out_base_path = r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK241\\CO3101_Do_an_Tri_tue_nhan_tao\\Output\"   # đường dẫn gốc tới folder\n","    \n","    # Fix the file path by adding the missing backslash or using os.path.join\n","    file_name = os.path.join(in_base_path, 'final_dataset_v1_afternb1.csv')  # Correct file path\n","    \n","    # Bắt đầu theo dõi thời gian\n","    t_start_time = time.time()\n","    \n","    # Load and preprocess data\n","    df_ds, train_essays, test_essays, val_essays = load_data(file_name)\n","    \n","    # Check the size of each set\n","    print(f'Full set size: {len(df_ds)}')\n","    print(f'Training set size: {len(train_essays)}')\n","    print(f'Validation set size: {len(val_essays)}')\n","    print(f'Test set size: {len(test_essays)}')\n","\n","    # ============================================================================================\n","    # Load the glove model\n","    # word2vec_output_file = get_tmpfile(r\"/kaggle/input/pdz-dath-ds/output_w2v.txt\")\n","    # word2vec_output_file = get_tmpfile(r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK241\\CO3101_Do_an_Tri_tue_nhan_tao\\Main\\Dataset\\output_w2v.txt\")\n","    # glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n","\n","    # Prepare train and validation embeddings\n","    # X_train = train_essays['processed_text_swr'].tolist()\n","    # X_val = val_essays['processed_text_swr'].tolist()\n","    # y_train = train_essays['label'].values\n","    # y_val = val_essays['label'].values\n","    \n","    # Prepare test data\n","    # X_test = test_essays['processed_text_swr'].tolist()\n","    # y_test = test_essays['label'].values\n","    \n","    # Embedding these information dataset\n","    # X_train_embeddings = np.array([sent2vec(sent, glove_model) for sent in X_train])\n","    # X_val_embeddings = np.array([sent2vec(sent, glove_model) for sent in X_val])\n","    \n","    # X_test_embeddings = np.array([sent2vec(sent, glove_model) for sent in X_test])\n","    # ============================================================================================\n","    # ============================================================================================\n","    # ============================================================================================\n","    # ============================================================================================\n","    # ============================================================================================\n","    # ============================================================================================\n","\n","    distilbert_model_path = os.path.join(out_base_path, 'distilbert_model')  # Use folder path without file extension\n","    distilbert_plot_file_path = os.path.join(out_base_path, 'distilbert_training_plot.png')\n","\n","    print(\"Training DistilBERT Model\")\n","    train_distilbert(df_ds, distilbert_model_path, distilbert_plot_file_path, n_epochs=10)\n","\n","    # Test the DistilBERT model on the test set\n","    print(\"Testing DistilBERT Model\")\n","    test_distilbert(test_essays, distilbert_model_path)\n","\n","    # ============================================================================================\n","    \n","    # Kết thúc theo dõi thời gian\n","    t_end_time = time.time()\n","    t_processing_time = t_end_time - t_start_time\n","\n","    # Convert minutes to hours and minutes\n","    t_hours = int(t_processing_time // 3600)  # Lấy số giờ\n","    t_minutes = int((t_processing_time % 3600) // 60)  # Lấy số phút\n","    t_seconds = int(t_processing_time % 60)  # Lấy số giây\n","\n","    logger.log_message(f\"Finished processing (total) in {t_hours} hours, {t_minutes} minutes, {t_seconds} seconds\")       \n","\n","if __name__ == \"__main__\":  \n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !rm -rf /kaggle/working/*"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !zip -r file.zip /kaggle/working"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5845037,"sourceId":9584960,"sourceType":"datasetVersion"},{"datasetId":5845480,"sourceId":9585533,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
