{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.9.6)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy) (1.25.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (4.2.0)\n",
      "Requirement already satisfied: promise in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow_datasets) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (17.0.0)\n",
      "Requirement already satisfied: simple-parsing in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: toml in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow_datasets) (4.66.5)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.9.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.4.5)\n",
      "Requirement already satisfied: zipp in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.20.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.65.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.25.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy seaborn scikit-learn tensorflow tensorflow_datasets \n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install sklearn\n",
    "%pip install xgboost\n",
    "%pip install gensim\n",
    "%pip install nltk\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Imports\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import joblib\n",
    "import torch\n",
    "import logging\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# ref: https://www.kaggle.com/code/eswarbabu88/toxic-comment-glove-logistic-regression\n",
    "# need to use glove_model from above\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================\n",
    "# Tạo class Logger\n",
    "class MyLogger:\n",
    "    def __init__(self, log_file='app.log'):\n",
    "        self.log_file = log_file\n",
    "        self._initialize_logger()\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "        # Check if the log file already exists; if so, append to it\n",
    "        if os.path.exists(self.log_file):\n",
    "            file_mode = 'a'  # Append mode\n",
    "        else:\n",
    "            file_mode = 'w'  # Write mode (create new file)\n",
    "\n",
    "        # Create a logger\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)  # Set logging level to INFO\n",
    "\n",
    "        # Create file handler\n",
    "        file_handler = logging.FileHandler(self.log_file, mode=file_mode, encoding='utf-8')\n",
    "        file_handler.setLevel(logging.INFO)  # Ensure the file handler logs INFO and above\n",
    "\n",
    "        # Create console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)  # Ensure the console handler logs INFO and above\n",
    "\n",
    "        # Set up the logging format\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        # Remove existing handlers to avoid duplicates\n",
    "        if self.logger.hasHandlers():\n",
    "            self.logger.handlers.clear()\n",
    "\n",
    "        # Add handlers to the logger\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def log_message(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def change_log_file(self, new_log_file):\n",
    "        \"\"\"Change the log file and reinitialize the logger.\"\"\"\n",
    "        self.log_file = new_log_file\n",
    "        self._initialize_logger()  # Reinitialize logger with the new log file\n",
    "\n",
    "# ============================================================================================\n",
    "# danh sách các biến toàn cục\n",
    "logger = MyLogger()\n",
    "\n",
    "# Enable/Disable tokenizers parallelism to avoid the warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# ============================================================================================\n",
    "# Danh sách các hàm xử lý\n",
    "# ============================================================================================\n",
    "# Load the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Remove URLs\n",
    "    text = re.sub(r'http\\S+|https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Step 2: Remove text in square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Step 3: Remove angle brackets\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    \n",
    "    # Step 4: Remove newlines, tabs, carriage returns, form feeds, backspace characters\n",
    "    text = re.sub(r'\\n|\\t|\\r|\\f|\\b', '', text)\n",
    "    \n",
    "    # Step 5: Remove words that contain numbers\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Step 6: Remove any non-alphanumeric characters, then make lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text).lower().strip()\n",
    "    \n",
    "    # Step 7: Tokenize the Vietnamese text using ViTokenizer\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    \n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load the dataset\n",
    "    df_ds = pd.read_csv(file_path)\n",
    "\n",
    "    # First, split into train and test sets (80% train, 20% test)\n",
    "    train_essays, test_essays = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Then, split the train set into train and validation sets (67% train, 33% validation)\n",
    "    train_essays, val_essays = train_test_split(train_essays, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return df_ds, train_essays, test_essays, val_essays\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    # Convert probabilities to binary predictions\n",
    "    binary_preds = (preds >= 0.5).astype(int)\n",
    "\n",
    "    # Compute ROC AUC score\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "\n",
    "    # Other metrics with zero_division set to handle undefined metrics\n",
    "    accuracy = accuracy_score(labels, binary_preds)\n",
    "    precision = precision_score(labels, binary_preds, zero_division=0)  # Use zero_division=0 to avoid warnings\n",
    "    recall = recall_score(labels, binary_preds)\n",
    "    f1 = f1_score(labels, binary_preds, zero_division=0)  # Use zero_division=0 to avoid warnings\n",
    "\n",
    "    return {\"roc_auc\": auc, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def compute_metrics_bert(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    \n",
    "    # Get the predicted class by selecting the class with the highest probability\n",
    "    preds = np.argmax(probs, axis=-1)\n",
    "    \n",
    "    # Compute ROC AUC score using probabilities of the positive class (class 1)\n",
    "    if len(np.unique(labels)) > 1:  # Ensure there are both classes present for AUC computation\n",
    "        auc = roc_auc_score(labels, probs[:, 1])\n",
    "    else:\n",
    "        auc = float('nan')  # AUC is not defined if only one class is present\n",
    "    \n",
    "    # Compute other metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, zero_division=0)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, zero_division=0)\n",
    "\n",
    "    return {\"roc_auc\": auc, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def plot_tsne(model, num):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.key_to_index:\n",
    "        if word not in stop_words:\n",
    "            tokens.append(np.array(model[word]))\n",
    "            labels.append(word)\n",
    "    tsne = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 2500, random_state = 23)\n",
    "    data = tsne.fit_transform(np.array(tokens[:num]))\n",
    "    x = []\n",
    "    y = []\n",
    "    for each in data:\n",
    "        x.append(each[0])\n",
    "        y.append(each[1])\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    for i in range(num):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy = (x[i], y[i]),\n",
    "                     xytext = (5,2),\n",
    "                     textcoords = 'offset points',\n",
    "                     ha = 'right',\n",
    "                     va = 'bottom')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming glove_model is already loaded in your environment\n",
    "# Function to convert a sentence to a vector\n",
    "def sent2vec(s, glove_model):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)  # This requires the 'punkt' tokenizer\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]  # Filter out non-alphabetic tokens\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(glove_model[w])  # Lookup word in GloVe model\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)  # Return a zero vector if no word embeddings are found\n",
    "    return v / np.sqrt((v ** 2).sum())  # Normalize the vector\n",
    "\n",
    "# ============================================================================================\n",
    "def plot_training_validation_curves(train_metrics, val_metrics, save_path, file_name):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Plot accuracy and loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_metrics['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(val_metrics['accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_metrics['loss'], label='Train Loss')\n",
    "    plt.plot(val_metrics['loss'], label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to the specified directory with the specified file name\n",
    "    plot_path = os.path.join(save_path, file_name)\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    # Show the plot as well\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to: {plot_path}\")\n",
    "\n",
    "def test_model(X_test_embeddings, y_test, model_file_path):\n",
    "    # Load the trained model\n",
    "    model = joblib.load(model_file_path)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    preds_test = model.predict_proba(X_test_embeddings)[:, 1]\n",
    "    \n",
    "    # Compute test metrics\n",
    "    test_loss = log_loss(y_test, preds_test)\n",
    "    test_metrics = compute_metrics(preds_test, y_test)\n",
    "\n",
    "    # Log test metrics\n",
    "    logger.log_message(\"Testing Results:\")\n",
    "    logger.log_message(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \")\n",
    "    logger.log_message(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \")\n",
    "    logger.log_message(f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Print test metrics for console\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \"\n",
    "          f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Return all test metrics\n",
    "    return {\"loss\": test_loss, **test_metrics}\n",
    "\n",
    "def test_linearsvc(X_test_embeddings, y_test, model_file_path):\n",
    "    # Load the trained model\n",
    "    model = joblib.load(model_file_path)\n",
    "    \n",
    "    # Predict decision function values (not probabilities) for the test set\n",
    "    preds_test = model.decision_function(X_test_embeddings)\n",
    "\n",
    "    # Since LinearSVC doesn't output probabilities, you can use the decision function output directly\n",
    "    # Compute the test loss using log loss (note: log_loss usually expects probabilities, but here we use decision values)\n",
    "    # You might want to threshold or normalize preds_test before passing it to log_loss\n",
    "    test_loss = log_loss(y_test, preds_test, labels=np.unique(y_test))  # Ensure labels are passed to handle binary classification\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_metrics = compute_metrics(preds_test, y_test)\n",
    "\n",
    "    # Log test metrics\n",
    "    logger.log_message(\"Testing Results:\")\n",
    "    logger.log_message(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \")\n",
    "    logger.log_message(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \")\n",
    "    logger.log_message(f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Print test metrics for console\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \"\n",
    "          f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Return all test metrics\n",
    "    return {\"loss\": test_loss, **test_metrics}\n",
    "\n",
    "def test_rnn(X_test, y_test, model_file_path):\n",
    "    \"\"\"\n",
    "    Function to test a trained RNN model using TensorFlow/Keras.\n",
    "    It loads the model, performs predictions, and computes metrics.\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model from file...\")\n",
    "        model = tf.keras.models.load_model(model_file_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_file_path}\")\n",
    "\n",
    "    # Create a TensorFlow dataset for testing\n",
    "    batch_size = 32\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "\n",
    "    # Predict logits (raw output before activation)\n",
    "    preds_test_logits = model.predict(test_dataset)\n",
    "\n",
    "    # Convert logits to probabilities using sigmoid since we are doing binary classification\n",
    "    preds_test = tf.sigmoid(preds_test_logits).numpy().flatten()\n",
    "\n",
    "    # Compute test loss using log loss (cross-entropy)\n",
    "    test_loss = log_loss(y_test, preds_test)\n",
    "\n",
    "    # Compute test metrics (precision, recall, F1, etc.)\n",
    "    test_metrics = compute_metrics(preds_test, y_test)\n",
    "\n",
    "    # Log test metrics\n",
    "    logger.log_message(\"Testing Results:\")\n",
    "    logger.log_message(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \")\n",
    "    logger.log_message(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \")\n",
    "    logger.log_message(f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "    # Print test metrics to console\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}, \"\n",
    "          f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}, Precision: {test_metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "    # Return test loss and all test metrics\n",
    "    return {\"loss\": test_loss, **test_metrics}\n",
    "\n",
    "def test_distilbert(X_test, model_file_path):\n",
    "    \"\"\"\n",
    "    Function to test a DistilBERT model on the test dataset and print metrics.\n",
    "    \"\"\"\n",
    "    # Load the trained model and tokenizer\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model and tokenizer from file...\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_file_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_file_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_file_path}\")\n",
    "\n",
    "    # Create the test dataset\n",
    "    test_dataset = Dataset.from_pandas(X_test)\n",
    "\n",
    "    # Tokenize the test dataset\n",
    "    tokenized_test = test_dataset.map(lambda examples: tokenizer(examples['processed_text_swr'], max_length=128, padding=True, truncation=True), batched=True)\n",
    "\n",
    "    # Initialize the Trainer (no training arguments needed for testing)\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Perform predictions on the test set\n",
    "    predictions = trainer.predict(tokenized_test)\n",
    "    preds_test = predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "    # Compute metrics for the test set using the logits and labels from predictions\n",
    "    test_metrics = compute_metrics(predictions.predictions, predictions.label_ids)\n",
    "\n",
    "    # Log and print the test metrics\n",
    "    logger.log_message(\"Testing Results:\")\n",
    "    logger.log_message(f\"Test Accuracy: {test_metrics['accuracy']:.4f}, Test ROC AUC: {test_metrics['roc_auc']:.4f}, \")\n",
    "    logger.log_message(f\"Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}, Test ROC AUC: {test_metrics['roc_auc']:.4f}, \"\n",
    "          f\"Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "    return test_metrics\n",
    "\n",
    "\n",
    "# ============================================================================================\n",
    "def shuffle_and_split(X, y, batch_size):\n",
    "    \"\"\"Shuffle the data and split into batches.\"\"\"\n",
    "    indices = np.random.permutation(len(y))  # Shuffle the indices\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    # Divide into batches\n",
    "    num_batches = len(y) // batch_size\n",
    "    X_batches = np.array_split(X_shuffled, num_batches)\n",
    "    y_batches = np.array_split(y_shuffled, num_batches)\n",
    "    \n",
    "    return X_batches, y_batches\n",
    "\n",
    "#######\n",
    "def train_logistic_regression(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n",
    "    logger.log_message(f\"Training a Logistic Regression model for {n_epochs} epochs...\")\n",
    "\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model from file...\")\n",
    "        model = joblib.load(model_file_path)\n",
    "    else:\n",
    "        print(\"Model not found. Training a new model...\")\n",
    "        model = LogisticRegression(max_iter=246, C=0.01, class_weight='balanced')\n",
    "\n",
    "        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n",
    "            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n",
    "\n",
    "            # Shuffle and split data into batches\n",
    "            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n",
    "\n",
    "            # Train model on each batch\n",
    "            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "                model.fit(X_batch, y_batch)\n",
    "\n",
    "            # After training on batches, validate\n",
    "            preds_train = model.predict_proba(X_train_embeddings)[:, 1]\n",
    "            preds_val = model.predict_proba(X_val_embeddings)[:, 1]\n",
    "\n",
    "            # Compute training metrics\n",
    "            train_loss = log_loss(y_train, preds_train)\n",
    "            train_metrics_epoch = compute_metrics(preds_train, y_train)\n",
    "            train_metrics['loss'].append(train_loss)\n",
    "            for key, value in train_metrics_epoch.items():\n",
    "                train_metrics[key].append(value)\n",
    "\n",
    "            # Compute validation metrics\n",
    "            val_loss = log_loss(y_val, preds_val)\n",
    "            val_metrics_epoch = compute_metrics(preds_val, y_val)\n",
    "            val_metrics['loss'].append(val_loss)\n",
    "            for key, value in val_metrics_epoch.items():\n",
    "                val_metrics[key].append(value)\n",
    "\n",
    "            # Print metrics for the epoch\n",
    "            # logger.log_message(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n",
    "            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n",
    "            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n",
    "\n",
    "        # Save the trained model\n",
    "        joblib.dump(model, model_file_path)\n",
    "        print(f\"Model saved to {model_file_path}\")\n",
    "\n",
    "        # Plot accuracy and loss curves\n",
    "        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"LogisticRegressionAnalysis.png\")\n",
    "\n",
    "    return model\n",
    "\n",
    "#######\n",
    "def train_xgboost(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n",
    "    logger.log_message(f\"Training an XGBoost model for {n_epochs} epochs...\")\n",
    "\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model from file...\")\n",
    "        model = joblib.load(model_file_path)\n",
    "    else:\n",
    "        print(\"Model not found. Training a new model...\")\n",
    "        # model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.01, n_estimators=100)\n",
    "        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.01, n_estimators=100)\n",
    "\n",
    "        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n",
    "            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n",
    "\n",
    "            # Shuffle and split data into batches\n",
    "            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n",
    "\n",
    "            # Train model on each batch\n",
    "            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "                model.fit(X_batch, y_batch)\n",
    "\n",
    "            # After training on batches, validate\n",
    "            preds_train = model.predict_proba(X_train_embeddings)[:, 1]\n",
    "            preds_val = model.predict_proba(X_val_embeddings)[:, 1]\n",
    "\n",
    "            # Compute training metrics\n",
    "            train_loss = log_loss(y_train, preds_train)\n",
    "            train_metrics_epoch = compute_metrics(preds_train, y_train)\n",
    "            train_metrics['loss'].append(train_loss)\n",
    "            for key, value in train_metrics_epoch.items():\n",
    "                train_metrics[key].append(value)\n",
    "\n",
    "            # Compute validation metrics\n",
    "            val_loss = log_loss(y_val, preds_val)\n",
    "            val_metrics_epoch = compute_metrics(preds_val, y_val)\n",
    "            val_metrics['loss'].append(val_loss)\n",
    "            for key, value in val_metrics_epoch.items():\n",
    "                val_metrics[key].append(value)\n",
    "\n",
    "            # Print metrics for the epoch\n",
    "            # logger.log_message(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n",
    "            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n",
    "            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n",
    "\n",
    "        # Save the trained model\n",
    "        joblib.dump(model, model_file_path)\n",
    "        print(f\"Model saved to {model_file_path}\")\n",
    "\n",
    "        # Plot accuracy and loss curves\n",
    "        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"xgboost_training_validation_curves.jpg\")\n",
    "\n",
    "    return model\n",
    "\n",
    "#######\n",
    "def train_random_forest(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n",
    "    logger.log_message(f\"Training a Random Forest model for {n_epochs} epochs...\")\n",
    "\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model from file...\")\n",
    "        model = joblib.load(model_file_path)\n",
    "    else:\n",
    "        print(\"Model not found. Training a new model...\")\n",
    "        model = RandomForestClassifier(n_estimators=24, max_depth=10, random_state=42)\n",
    "\n",
    "        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n",
    "            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n",
    "\n",
    "            # Shuffle and split data into batches\n",
    "            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n",
    "\n",
    "            # Train model on each batch\n",
    "            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "                start = time.time()\n",
    "                model.fit(X_batch, y_batch)\n",
    "                # print(f\"Model fit time: {time.time() - start} seconds\")\n",
    "\n",
    "            # After training on batches, validate\n",
    "            preds_train = model.predict_proba(X_train_embeddings)[:, 1]\n",
    "            preds_val = model.predict_proba(X_val_embeddings)[:, 1]\n",
    "\n",
    "            # Compute training metrics\n",
    "            train_loss = log_loss(y_train, preds_train)\n",
    "            train_metrics_epoch = compute_metrics(preds_train, y_train)\n",
    "            train_metrics['loss'].append(train_loss)\n",
    "            for key, value in train_metrics_epoch.items():\n",
    "                train_metrics[key].append(value)\n",
    "\n",
    "            # Compute validation metrics\n",
    "            val_loss = log_loss(y_val, preds_val)\n",
    "            val_metrics_epoch = compute_metrics(preds_val, y_val)\n",
    "            val_metrics['loss'].append(val_loss)\n",
    "            for key, value in val_metrics_epoch.items():\n",
    "                val_metrics[key].append(value)\n",
    "\n",
    "            # Print metrics for the epoch\n",
    "            # logger.log_message(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n",
    "            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n",
    "            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n",
    "\n",
    "        # Save the trained model\n",
    "        joblib.dump(model, model_file_path)\n",
    "        print(f\"Model saved to {model_file_path}\")\n",
    "\n",
    "        # Plot accuracy and loss curves\n",
    "        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"RF_training_validation_curves.jpg\")\n",
    "\n",
    "    return model\n",
    "\n",
    "#######\n",
    "def train_linear_svc(X_train_embeddings, y_train, X_val_embeddings, y_val, model_file_path, out_base_path, n_epochs=20, batch_size=32):\n",
    "    logger.log_message(f\"Training a LinearSVC model for {n_epochs} epochs...\")\n",
    "\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model from file...\")\n",
    "        model = joblib.load(model_file_path)\n",
    "    else:\n",
    "        print(\"Model not found. Training a new model...\")\n",
    "        # Instantiate the LinearSVC model\n",
    "        model = LinearSVC(max_iter=246, class_weight='balanced', tol=1e-4, C=1.0)\n",
    "\n",
    "        train_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "        val_metrics = {'accuracy': [], 'loss': [], 'roc_auc': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Epochs\", unit=\"epoch\"):\n",
    "            logger.log_message(f\"Epoch {epoch + 1}/{n_epochs} - Shuffling and batching data\")\n",
    "\n",
    "            # Shuffle and split data into batches\n",
    "            X_train_batches, y_train_batches = shuffle_and_split(X_train_embeddings, y_train, batch_size)\n",
    "\n",
    "            # Train model on each batch\n",
    "            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "                model.fit(X_batch, y_batch)\n",
    "\n",
    "            # After training on batches, validate\n",
    "            preds_train = model.decision_function(X_train_embeddings)\n",
    "            preds_val = model.decision_function(X_val_embeddings)\n",
    "\n",
    "            # Compute training metrics\n",
    "            train_loss = log_loss(y_train, preds_train)\n",
    "            train_metrics_epoch = compute_metrics(preds_train, y_train)\n",
    "            train_metrics['loss'].append(train_loss)\n",
    "            for key, value in train_metrics_epoch.items():\n",
    "                train_metrics[key].append(value)\n",
    "\n",
    "            # Compute validation metrics\n",
    "            val_loss = log_loss(y_val, preds_val)\n",
    "            val_metrics_epoch = compute_metrics(preds_val, y_val)\n",
    "            val_metrics['loss'].append(val_loss)\n",
    "            for key, value in val_metrics_epoch.items():\n",
    "                val_metrics[key].append(value)\n",
    "\n",
    "            # Print metrics for the epoch\n",
    "            logger.log_message(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \")\n",
    "            logger.log_message(f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \")\n",
    "            logger.log_message(f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Train ROC AUC: {train_metrics_epoch['roc_auc']:.4f}, Precision: {train_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {train_metrics_epoch['recall']:.4f}, F1: {train_metrics_epoch['f1']:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics_epoch['accuracy']:.4f}, \"\n",
    "                  f\"Val ROC AUC: {val_metrics_epoch['roc_auc']:.4f}, Precision: {val_metrics_epoch['precision']:.4f}, \"\n",
    "                  f\"Recall: {val_metrics_epoch['recall']:.4f}, F1: {val_metrics_epoch['f1']:.4f}\")\n",
    "\n",
    "        # Save the trained model\n",
    "        joblib.dump(model, model_file_path)\n",
    "        print(f\"Model saved to {model_file_path}\")\n",
    "\n",
    "        # Plot accuracy and loss curves\n",
    "        plot_training_validation_curves(train_metrics, val_metrics, out_base_path, \"LinearSVC_training_validation_curves.png\")\n",
    "\n",
    "    return model\n",
    "\n",
    "#######\n",
    "def train_rnn(X_train, y_train, X_val, y_val, model_file_path, plot_file_path, n_epochs=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Function to train a Bidirectional LSTM RNN using TensorFlow/Keras.\n",
    "    It checks if the model already exists, otherwise it trains and saves the model.\n",
    "    \"\"\"\n",
    "    # Create TensorFlow datasets from the input data\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "    # Check if the model already exists\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model from file...\")\n",
    "        model = tf.keras.models.load_model(model_file_path)\n",
    "    else:\n",
    "        print(\"Model not found. Training a new model...\")\n",
    "\n",
    "        # Text vectorization layer\n",
    "        VOCAB_SIZE = 1000\n",
    "        encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "        encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "        # Define the Bidirectional LSTM RNN model\n",
    "        model = tf.keras.Sequential([\n",
    "            encoder,\n",
    "            tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True),\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                      metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_dataset, epochs=n_epochs, validation_data=val_dataset)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(model_file_path)\n",
    "        print(f\"Model saved to {model_file_path}\")\n",
    "\n",
    "        # Plot training and validation accuracy and loss\n",
    "        def plot_graphs(history, metric):\n",
    "            plt.plot(history.history[metric])\n",
    "            plt.plot(history.history['val_' + metric], '')\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(metric)\n",
    "            plt.legend([metric, 'val_' + metric])\n",
    "\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plot_graphs(history, 'accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plot_graphs(history, 'loss')\n",
    "        plt.ylim(0, None)\n",
    "\n",
    "        # Save the plot\n",
    "        plt.savefig(plot_file_path)\n",
    "        print(f\"Training plot saved to {plot_file_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "#######\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"processed_text_swr\"], max_length=128, padding=True, truncation=True)\n",
    "\n",
    "def plot_graphs(log_history, metric):\n",
    "    epochs = [entry['epoch'] for entry in log_history if metric in entry]\n",
    "    metric_values = [entry[metric] for entry in log_history if metric in entry]\n",
    "    plt.plot(epochs, metric_values)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric])\n",
    "\n",
    "def train_distilbert(df_ds, model_file_path, image_file_path, n_epochs=10):\n",
    "    \"\"\"\n",
    "    Function to train a DistilBERT model. \n",
    "    It checks if the model already exists, otherwise it trains and saves the model.\n",
    "    \"\"\"\n",
    "    # Check if model exists\n",
    "    if os.path.exists(model_file_path):\n",
    "        print(\"Loading model from file...\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_file_path)\n",
    "    else:\n",
    "        print(\"Model not found. Training a new model...\")\n",
    "\n",
    "        # Split dataset\n",
    "        train_essays, test_essays = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "        train_essays, val_essays = train_test_split(train_essays, test_size=0.33, random_state=42)\n",
    "\n",
    "        # Tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "        # Create dataset\n",
    "        train_essay_dataset = Dataset.from_pandas(train_essays)\n",
    "        val_essay_dataset = Dataset.from_pandas(val_essays)\n",
    "        \n",
    "        # Tokenize datasets, passing the tokenizer as an argument to the map function\n",
    "        tokenized_train_essays = train_essay_dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True)\n",
    "        tokenized_val_essays = val_essay_dataset.map(lambda examples: preprocess_function(examples, tokenizer), batched=True)\n",
    "\n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"/kaggle/working/\",\n",
    "            learning_rate=2e-5,\n",
    "            num_train_epochs=n_epochs,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            report_to='none'\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train_essays,\n",
    "            eval_dataset=tokenized_val_essays,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics_bert\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the trained model\n",
    "        model.save_pretrained(model_file_path)  # Updated to Hugging Face save method\n",
    "        tokenizer.save_pretrained(model_file_path)  # Save tokenizer too\n",
    "        print(f\"Model saved to {model_file_path}\")\n",
    "\n",
    "        # Plot accuracy and loss graphs\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plot_graphs(trainer.state.log_history, 'eval_accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plot_graphs(trainer.state.log_history, 'eval_loss')\n",
    "        plt.ylim(0, None)\n",
    "\n",
    "        # Save the plot\n",
    "        plt.savefig(image_file_path)\n",
    "        plt.show()\n",
    "        print(f\"Training plot saved as {image_file_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Khởi tạo số luồng xử lý song song\n",
    "    # max_workers = 1 \n",
    "    \n",
    "    # kaggle \n",
    "    # in_base_path = r\"/kaggle/input/pdz-dath-ds/\"\n",
    "    # out_base_path = r\"/kaggle/working/\"\n",
    "    \n",
    "    in_base_path = r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK241\\CO3101_Do_an_Tri_tue_nhan_tao\\Main\\Dataset\"\n",
    "    out_base_path = r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK241\\CO3101_Do_an_Tri_tue_nhan_tao\\Output\"   # đường dẫn gốc tới folder\n",
    "    \n",
    "    # Fix the file path by adding the missing backslash or using os.path.join\n",
    "    file_name = os.path.join(in_base_path, 'final_dataset_v1_afternb1.csv')  # Correct file path\n",
    "    \n",
    "    # Bắt đầu theo dõi thời gian\n",
    "    t_start_time = time.time()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df_ds, train_essays, test_essays, val_essays = load_data(file_name)\n",
    "    \n",
    "    # Check the size of each set\n",
    "    print(f'Full set size: {len(df_ds)}')\n",
    "    print(f'Training set size: {len(train_essays)}')\n",
    "    print(f'Validation set size: {len(val_essays)}')\n",
    "    print(f'Test set size: {len(test_essays)}')\n",
    "\n",
    "    # ============================================================================================\n",
    "    # Load the glove model\n",
    "    # word2vec_output_file = get_tmpfile(r\"/kaggle/input/pdz-dath-ds/output_w2v.txt\")\n",
    "    word2vec_output_file = get_tmpfile(r\"E:\\2_LEARNING_BKU\\2_File_2\\K22_HK241\\CO3101_Do_an_Tri_tue_nhan_tao\\Main\\Dataset\\output_w2v.txt\")\n",
    "    glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "    # Prepare train and validation embeddings\n",
    "    X_train = train_essays['processed_text_swr'].tolist()\n",
    "    X_val = val_essays['processed_text_swr'].tolist()\n",
    "    y_train = train_essays['label'].values\n",
    "    y_val = val_essays['label'].values\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_essays['processed_text_swr'].tolist()\n",
    "    y_test = test_essays['label'].values\n",
    "    \n",
    "    # Embedding these information dataset\n",
    "    X_train_embeddings = np.array([sent2vec(sent, glove_model) for sent in X_train])\n",
    "    X_val_embeddings = np.array([sent2vec(sent, glove_model) for sent in X_val])\n",
    "    \n",
    "    X_test_embeddings = np.array([sent2vec(sent, glove_model) for sent in X_test])\n",
    "    # ============================================================================================\n",
    "\n",
    "    # Train the Logistic Regression model using the train and validation sets\n",
    "    logistic_model_file_path = os.path.join(out_base_path, 'logistic_regression_model.pkl')\n",
    "    train_logistic_regression(X_train_embeddings, y_train, X_val_embeddings, y_val, logistic_model_file_path, out_base_path)\n",
    "\n",
    "    # Test the Logistic Regression model on the test set\n",
    "    print(\"Testing Logistic Regression Model\")\n",
    "    test_model(X_test_embeddings, y_test, logistic_model_file_path)\n",
    "    # ============================================================================================\n",
    "\n",
    "    # Train the XGBoost model using the train and validation sets\n",
    "    xgboost_model_file_path = os.path.join(out_base_path, 'xgboost_model.pkl')\n",
    "    train_xgboost(X_train_embeddings, y_train, X_val_embeddings, y_val, xgboost_model_file_path, out_base_path)\n",
    "\n",
    "    # Test the XGBoost model on the test set\n",
    "    print(\"Testing XGBoost Model\")\n",
    "    test_model(X_test_embeddings, y_test, xgboost_model_file_path)\n",
    "    # ============================================================================================\n",
    "    \n",
    "    # Train the Random Forest model using the train and validation sets\n",
    "    randomforest_model_file_path = os.path.join(out_base_path, 'randomforest_model.pkl')\n",
    "    train_random_forest(X_train_embeddings, y_train, X_val_embeddings, y_val, randomforest_model_file_path, out_base_path)\n",
    "\n",
    "    # Test the Random Forest model on the test set\n",
    "    print(\"Testing Random Forest Model\")\n",
    "    test_model(X_test_embeddings, y_test, randomforest_model_file_path)\n",
    "    \n",
    "    \n",
    "    # ============================================================================================\n",
    "    \n",
    "    # Train the Linear SVC model using the train and validation sets\n",
    "    linear_svc_model_file_path = os.path.join(out_base_path, 'linearsvc_model.pkl')\n",
    "    train_linear_svc(X_train_embeddings, y_train, X_val_embeddings, y_val, linear_svc_model_file_path, out_base_path)\n",
    "\n",
    "    # Test the Linear SVC model on the test set\n",
    "    print(\"Testing linear SVC Model\")\n",
    "    test_linearsvc(X_test_embeddings, y_test, linear_svc_model_file_path)\n",
    "    \n",
    "    \n",
    "    # ============================================================================================\n",
    "    # Train the Bidirectional LSTM RNN model using the train and validation sets\n",
    "    # rnn_model_file_path = os.path.join(out_base_path, 'tf_lstm_rnn.keras')\n",
    "    # rnn_plot_file_path = os.path.join(out_base_path, 'tf_lstm_rnn.png')\n",
    "\n",
    "    # print(\"Training RNN Model\")\n",
    "    # train_rnn(X_train, y_train, X_val, y_val, rnn_model_file_path, rnn_plot_file_path, n_epochs=20, batch_size=32)\n",
    "\n",
    "    # # Test the RNN model on the test set\n",
    "    # print(\"Testing RNN Model\")\n",
    "    # test_rnn(X_test, y_test, rnn_model_file_path)\n",
    "    \n",
    "    # Train the DistilBERT model using the train and validation sets\n",
    "    # distilbert_model_path = os.path.join(out_base_path, 'distilbert_model')  # Use folder path without file extension\n",
    "    # distilbert_plot_file_path = os.path.join(out_base_path, 'distilbert_training_plot.png')\n",
    "\n",
    "    # print(\"Training DistilBERT Model\")\n",
    "    # train_distilbert(df_ds, distilbert_model_path, distilbert_plot_file_path, n_epochs=10)\n",
    "\n",
    "    # # Test the DistilBERT model on the test set\n",
    "    # print(\"Testing DistilBERT Model\")\n",
    "    # test_distilbert(test_essays, distilbert_model_path)\n",
    "\n",
    "\n",
    "    # ============================================================================================\n",
    "    # Train the DistilBERT model using the train and validation sets\n",
    "    distilbert_model_file_path = os.path.join(out_base_path, 'distilbert_model.pkl')\n",
    "    distilbert_plot_file_path = os.path.join(out_base_path, 'distilbert_training_plot.png')\n",
    "\n",
    "    print(\"Training DistilBERT Model\")\n",
    "    train_distilbert(df_ds, distilbert_model_file_path, distilbert_plot_file_path, n_epochs=10)\n",
    "\n",
    "    # Test the DistilBERT model on the test set\n",
    "    print(\"Testing DistilBERT Model\")\n",
    "    test_distilbert(test_essays, distilbert_model_file_path)\n",
    "\n",
    "    # ============================================================================================\n",
    "\n",
    "    # Kết thúc theo dõi thời gian\n",
    "    t_end_time = time.time()\n",
    "    t_processing_time = t_end_time - t_start_time\n",
    "\n",
    "    # Convert minutes to hours and minutes\n",
    "    t_hours = int(t_processing_time // 3600)  # Lấy số giờ\n",
    "    t_minutes = int((t_processing_time % 3600) // 60)  # Lấy số phút\n",
    "    t_seconds = int(t_processing_time % 60)  # Lấy số giây\n",
    "\n",
    "    logger.log_message(f\"Finished processing (total) in {t_hours} hours, {t_minutes} minutes, {t_seconds} seconds\")       \n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
